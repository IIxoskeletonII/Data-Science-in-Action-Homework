{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c46319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# HOMEWORK 1: Data Collection and Analysis\n",
    "# Student Name: [Your Name]\n",
    "# City: London\n",
    "# Data Source: Real REST APIs (OpenWeather, OpenAQ, London Datastore)\n",
    "# ==========================================\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization Settings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663ba09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "CITY = \"London\"\n",
    "LAT = 51.5074\n",
    "LON = -0.1278\n",
    "\n",
    "# Time Period: Jan 1, 2024 to June 30, 2024\n",
    "START_DATE = datetime(2024, 1, 1)\n",
    "END_DATE = datetime(2024, 6, 30)\n",
    "\n",
    "# API KEYS\n",
    "OPENWEATHER_API_KEY = \"325b69ff5a70f7ff71ba31d1474d270f\"\n",
    "OPENAQ_API_KEY = \"2f891ea1344c958c5486d58aca8fe88b3b02e01043140eb06847fb8e1f4cd75b\"\n",
    "\n",
    "# File Paths (Local storage)\n",
    "WEATHER_FILE = \"london_weather_real.csv\"\n",
    "AIR_QUALITY_FILE = \"london_aq_real.csv\"\n",
    "MOBILITY_FILE = \"london_mobility_real.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c65b009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Weather Collection for London (2024-01-01 to 2024-06-30)...\n",
      "Skipping 2024-01-01: API Status 401\n",
      "Skipping 2024-01-02: API Status 401\n",
      "Skipping 2024-01-03: API Status 401\n",
      "Skipping 2024-01-04: API Status 401\n",
      "Skipping 2024-01-05: API Status 401\n",
      "Skipping 2024-01-06: API Status 401\n",
      "Skipping 2024-01-07: API Status 401\n",
      "Skipping 2024-01-08: API Status 401\n",
      "Skipping 2024-01-09: API Status 401\n",
      "Skipping 2024-01-10: API Status 401\n",
      "Skipping 2024-01-11: API Status 401\n",
      "Skipping 2024-01-12: API Status 401\n",
      "Skipping 2024-01-13: API Status 401\n",
      "Skipping 2024-01-14: API Status 401\n",
      "Skipping 2024-01-15: API Status 401\n",
      "Skipping 2024-01-16: API Status 401\n",
      "Skipping 2024-01-17: API Status 401\n",
      "Skipping 2024-01-18: API Status 401\n",
      "Skipping 2024-01-19: API Status 401\n",
      "Skipping 2024-01-20: API Status 401\n",
      "Skipping 2024-01-21: API Status 401\n",
      "Skipping 2024-01-22: API Status 401\n",
      "Skipping 2024-01-23: API Status 401\n",
      "Skipping 2024-01-24: API Status 401\n",
      "Skipping 2024-01-25: API Status 401\n",
      "Skipping 2024-01-26: API Status 401\n",
      "Skipping 2024-01-27: API Status 401\n",
      "Skipping 2024-01-28: API Status 401\n",
      "Skipping 2024-01-29: API Status 401\n",
      "Skipping 2024-01-30: API Status 401\n",
      "Skipping 2024-01-31: API Status 401\n",
      "Skipping 2024-02-01: API Status 401\n",
      "Skipping 2024-02-02: API Status 401\n",
      "Skipping 2024-02-03: API Status 401\n",
      "Skipping 2024-02-04: API Status 401\n",
      "Skipping 2024-02-05: API Status 401\n",
      "Skipping 2024-02-06: API Status 401\n",
      "Skipping 2024-02-07: API Status 401\n",
      "Skipping 2024-02-08: API Status 401\n",
      "Skipping 2024-02-09: API Status 401\n",
      "Skipping 2024-02-10: API Status 401\n",
      "Skipping 2024-02-11: API Status 401\n",
      "Skipping 2024-02-12: API Status 401\n",
      "Skipping 2024-02-13: API Status 401\n",
      "Skipping 2024-02-14: API Status 401\n",
      "Skipping 2024-02-15: API Status 401\n",
      "Skipping 2024-02-16: API Status 401\n",
      "Skipping 2024-02-17: API Status 401\n",
      "Skipping 2024-02-18: API Status 401\n",
      "Skipping 2024-02-19: API Status 401\n",
      "Skipping 2024-02-20: API Status 401\n",
      "Skipping 2024-02-21: API Status 401\n",
      "Skipping 2024-02-22: API Status 401\n",
      "Skipping 2024-02-23: API Status 401\n",
      "Skipping 2024-02-24: API Status 401\n",
      "Skipping 2024-02-25: API Status 401\n",
      "Skipping 2024-02-26: API Status 401\n",
      "Skipping 2024-02-27: API Status 401\n",
      "Skipping 2024-02-28: API Status 401\n",
      "Skipping 2024-02-29: API Status 401\n",
      "Skipping 2024-03-01: API Status 401\n",
      "Skipping 2024-03-02: API Status 401\n",
      "Skipping 2024-03-03: API Status 401\n",
      "Skipping 2024-03-04: API Status 401\n",
      "Skipping 2024-03-05: API Status 401\n",
      "Skipping 2024-03-06: API Status 401\n",
      "Skipping 2024-03-07: API Status 401\n",
      "Skipping 2024-03-08: API Status 401\n",
      "Skipping 2024-03-09: API Status 401\n",
      "Skipping 2024-03-10: API Status 401\n",
      "Skipping 2024-03-11: API Status 401\n",
      "Skipping 2024-03-12: API Status 401\n",
      "Skipping 2024-03-13: API Status 401\n",
      "Skipping 2024-03-14: API Status 401\n",
      "Skipping 2024-03-15: API Status 401\n",
      "Skipping 2024-03-16: API Status 401\n",
      "Skipping 2024-03-17: API Status 401\n",
      "Skipping 2024-03-18: API Status 401\n",
      "Skipping 2024-03-19: API Status 401\n",
      "Skipping 2024-03-20: API Status 401\n",
      "Skipping 2024-03-21: API Status 401\n",
      "Skipping 2024-03-22: API Status 401\n",
      "Skipping 2024-03-23: API Status 401\n",
      "Skipping 2024-03-24: API Status 401\n",
      "Skipping 2024-03-25: API Status 401\n",
      "Skipping 2024-03-26: API Status 401\n",
      "Skipping 2024-03-27: API Status 401\n",
      "Skipping 2024-03-28: API Status 401\n",
      "Skipping 2024-03-29: API Status 401\n",
      "Skipping 2024-03-30: API Status 401\n",
      "Skipping 2024-03-31: API Status 401\n",
      "Skipping 2024-04-01: API Status 401\n",
      "Skipping 2024-04-02: API Status 401\n",
      "Skipping 2024-04-03: API Status 401\n",
      "Skipping 2024-04-04: API Status 401\n",
      "Skipping 2024-04-05: API Status 401\n",
      "Skipping 2024-04-06: API Status 401\n",
      "Skipping 2024-04-07: API Status 401\n",
      "Skipping 2024-04-08: API Status 401\n",
      "Skipping 2024-04-09: API Status 401\n",
      "Skipping 2024-04-10: API Status 401\n",
      "Skipping 2024-04-11: API Status 401\n",
      "Skipping 2024-04-12: API Status 401\n",
      "Skipping 2024-04-13: API Status 401\n",
      "Skipping 2024-04-14: API Status 401\n",
      "Skipping 2024-04-15: API Status 401\n",
      "Skipping 2024-04-16: API Status 401\n",
      "Skipping 2024-04-17: API Status 401\n",
      "Skipping 2024-04-18: API Status 401\n",
      "Skipping 2024-04-19: API Status 401\n",
      "Skipping 2024-04-20: API Status 401\n",
      "Skipping 2024-04-21: API Status 401\n",
      "Skipping 2024-04-22: API Status 401\n",
      "Skipping 2024-04-23: API Status 401\n",
      "Skipping 2024-04-24: API Status 401\n",
      "Skipping 2024-04-25: API Status 401\n",
      "Skipping 2024-04-26: API Status 401\n",
      "Skipping 2024-04-27: API Status 401\n",
      "Skipping 2024-04-28: API Status 401\n",
      "Skipping 2024-04-29: API Status 401\n",
      "Skipping 2024-04-30: API Status 401\n",
      "Skipping 2024-05-01: API Status 401\n",
      "Skipping 2024-05-02: API Status 401\n",
      "Skipping 2024-05-03: API Status 401\n",
      "Skipping 2024-05-04: API Status 401\n",
      "Skipping 2024-05-05: API Status 401\n",
      "Skipping 2024-05-06: API Status 401\n",
      "Skipping 2024-05-07: API Status 401\n",
      "Skipping 2024-05-08: API Status 401\n",
      "Skipping 2024-05-09: API Status 401\n",
      "Skipping 2024-05-10: API Status 401\n",
      "Skipping 2024-05-11: API Status 401\n",
      "Skipping 2024-05-12: API Status 401\n",
      "Skipping 2024-05-13: API Status 401\n",
      "Skipping 2024-05-14: API Status 401\n",
      "Skipping 2024-05-15: API Status 401\n",
      "Skipping 2024-05-16: API Status 401\n",
      "Skipping 2024-05-17: API Status 401\n",
      "Skipping 2024-05-18: API Status 401\n",
      "Skipping 2024-05-19: API Status 401\n",
      "Skipping 2024-05-20: API Status 401\n",
      "Skipping 2024-05-21: API Status 401\n",
      "Skipping 2024-05-22: API Status 401\n",
      "Skipping 2024-05-23: API Status 401\n",
      "Skipping 2024-05-24: API Status 401\n",
      "Skipping 2024-05-25: API Status 401\n",
      "Skipping 2024-05-26: API Status 401\n",
      "Skipping 2024-05-27: API Status 401\n",
      "Skipping 2024-05-28: API Status 401\n",
      "Skipping 2024-05-29: API Status 401\n",
      "Skipping 2024-05-30: API Status 401\n",
      "Skipping 2024-05-31: API Status 401\n",
      "Skipping 2024-06-01: API Status 401\n",
      "Skipping 2024-06-02: API Status 401\n",
      "Skipping 2024-06-03: API Status 401\n",
      "Skipping 2024-06-04: API Status 401\n",
      "Skipping 2024-06-05: API Status 401\n",
      "Skipping 2024-06-06: API Status 401\n",
      "Skipping 2024-06-07: API Status 401\n",
      "Skipping 2024-06-08: API Status 401\n",
      "Skipping 2024-06-09: API Status 401\n",
      "Skipping 2024-06-10: API Status 401\n",
      "Skipping 2024-06-11: API Status 401\n",
      "Skipping 2024-06-12: API Status 401\n",
      "Skipping 2024-06-13: API Status 401\n",
      "Skipping 2024-06-14: API Status 401\n",
      "Skipping 2024-06-15: API Status 401\n",
      "Skipping 2024-06-16: API Status 401\n",
      "Skipping 2024-06-17: API Status 401\n",
      "Skipping 2024-06-18: API Status 401\n",
      "Skipping 2024-06-19: API Status 401\n",
      "Skipping 2024-06-20: API Status 401\n",
      "Skipping 2024-06-21: API Status 401\n",
      "Skipping 2024-06-22: API Status 401\n",
      "Skipping 2024-06-23: API Status 401\n",
      "Skipping 2024-06-24: API Status 401\n",
      "Skipping 2024-06-25: API Status 401\n",
      "Skipping 2024-06-26: API Status 401\n",
      "Skipping 2024-06-27: API Status 401\n",
      "Skipping 2024-06-28: API Status 401\n",
      "Skipping 2024-06-29: API Status 401\n",
      "Skipping 2024-06-30: API Status 401\n",
      "Failure: No weather data collected. Check API Key subscription type.\n"
     ]
    }
   ],
   "source": [
    "def fetch_weather_history():\n",
    "    print(f\"Starting Weather Collection for {CITY} ({START_DATE.date()} to {END_DATE.date()})...\")\n",
    "    \n",
    "    url = \"https://api.openweathermap.org/data/3.0/onecall/timemachine\"\n",
    "    data_records = []\n",
    "    current_date = START_DATE\n",
    "    \n",
    "    while current_date <= END_DATE:\n",
    "        # Unix timestamp for the specific day at noon\n",
    "        dt_stamp = int(current_date.replace(hour=12, minute=0).timestamp())\n",
    "        \n",
    "        params = {\n",
    "            'lat': LAT,\n",
    "            'lon': LON,\n",
    "            'dt': dt_stamp,\n",
    "            'appid': OPENWEATHER_API_KEY,\n",
    "            'units': 'metric'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                # 'data' is a list of hourly entries for that requested time\n",
    "                # We take the first one (closest to noon)\n",
    "                if 'data' in resp_json and len(resp_json['data']) > 0:\n",
    "                    day_data = resp_json['data'][0]\n",
    "                    data_records.append({\n",
    "                        'date': current_date.strftime('%Y-%m-%d'),\n",
    "                        'temp': day_data.get('temp'),\n",
    "                        'humidity': day_data.get('humidity'),\n",
    "                        'wind_speed': day_data.get('wind_speed'),\n",
    "                        'pressure': day_data.get('pressure'),\n",
    "                        'condition': day_data['weather'][0]['main'] if 'weather' in day_data else 'Unknown'\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"Skipping {current_date.date()}: API Status {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error on {current_date.date()}: {e}\")\n",
    "        \n",
    "        # Advance one day\n",
    "        current_date += timedelta(days=1)\n",
    "        # Polite delay\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    # Save\n",
    "    if data_records:\n",
    "        df = pd.DataFrame(data_records)\n",
    "        df.to_csv(WEATHER_FILE, index=False)\n",
    "        print(f\"Success: Saved {len(df)} weather records to {WEATHER_FILE}\")\n",
    "    else:\n",
    "        print(\"Failure: No weather data collected. Check API Key subscription type.\")\n",
    "\n",
    "# Execute Collection\n",
    "fetch_weather_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9f1def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Air Quality Collection for London...\n",
      "API Error: 410 - {\"message\": \"Gone. Version 1 and Version 2 API endpoints are retired and no longer available. Please migrate to Version 3 endpoints.\"}\n"
     ]
    }
   ],
   "source": [
    "def fetch_air_quality():\n",
    "    print(f\"Starting Air Quality Collection for {CITY}...\")\n",
    "    \n",
    "    url = \"https://api.openaq.org/v2/measurements\"\n",
    "    headers = {\"X-API-Key\": OPENAQ_API_KEY}\n",
    "    \n",
    "    params = {\n",
    "        'city': 'London',\n",
    "        'country': 'GB',\n",
    "        'parameter': 'pm25',\n",
    "        'date_from': START_DATE.strftime('%Y-%m-%d'),\n",
    "        'date_to': END_DATE.strftime('%Y-%m-%d'),\n",
    "        'limit': 10000, \n",
    "        'order_by': 'datetime'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get('results', [])\n",
    "            records = []\n",
    "            \n",
    "            for item in results:\n",
    "                # We extract the UTC date and value\n",
    "                records.append({\n",
    "                    'date': item['date']['utc'][:10], # Extract YYYY-MM-DD\n",
    "                    'pm25': item['value'],\n",
    "                    'unit': item['unit']\n",
    "                })\n",
    "            \n",
    "            if records:\n",
    "                df = pd.DataFrame(records)\n",
    "                # Group by date to get daily average (sensors report hourly)\n",
    "                df_daily = df.groupby('date')['pm25'].mean().reset_index()\n",
    "                df_daily.to_csv(AIR_QUALITY_FILE, index=False)\n",
    "                print(f\"Success: Saved {len(df_daily)} daily air quality records to {AIR_QUALITY_FILE}\")\n",
    "            else:\n",
    "                print(\"Warning: No records found for this date range/city.\")\n",
    "        else:\n",
    "            print(f\"API Error: {response.status_code} - {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "\n",
    "# Execute Collection\n",
    "fetch_air_quality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7502b5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching TfL Mobility Data...\n",
      "Error fetching mobility data: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "def fetch_mobility():\n",
    "    print(\"Fetching TfL Mobility Data...\")\n",
    "    # Direct CSV link for TfL Journeys (Publicly available, reliable URL)\n",
    "    csv_url = \"https://data.london.gov.uk/download/tfl-journeys-type/f3d9796d-37bf-42a3-92c1-c4269a6572eb/tfl-journeys-type.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_url)\n",
    "        \n",
    "        # Rename columns to standardized names\n",
    "        # The dataset usually has 'Period and Financial Year' or 'Reporting Period'\n",
    "        df.to_csv(MOBILITY_FILE, index=False)\n",
    "        print(f\"Success: Downloaded mobility data to {MOBILITY_FILE}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching mobility data: {e}\")\n",
    "\n",
    "# Execute Collection\n",
    "fetch_mobility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5be5bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and Merging Data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'london_weather_real.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing and Merging Data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Load Data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df_weather = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWEATHER_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df_aq = pd.read_csv(AIR_QUALITY_FILE)\n\u001b[32m      6\u001b[39m df_mobility = pd.read_csv(MOBILITY_FILE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eliya Allam\\Documents\\University\\Data Science and Management MSc\\Data Science in Action\\Homework 1\\Data-Science-in-Action-Homework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eliya Allam\\Documents\\University\\Data Science and Management MSc\\Data Science in Action\\Homework 1\\Data-Science-in-Action-Homework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eliya Allam\\Documents\\University\\Data Science and Management MSc\\Data Science in Action\\Homework 1\\Data-Science-in-Action-Homework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eliya Allam\\Documents\\University\\Data Science and Management MSc\\Data Science in Action\\Homework 1\\Data-Science-in-Action-Homework\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Eliya Allam\\Documents\\University\\Data Science and Management MSc\\Data Science in Action\\Homework 1\\Data-Science-in-Action-Homework\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'london_weather_real.csv'"
     ]
    }
   ],
   "source": [
    "print(\"Processing and Merging Data...\")\n",
    "\n",
    "# 1. Load Data\n",
    "df_weather = pd.read_csv(WEATHER_FILE)\n",
    "df_aq = pd.read_csv(AIR_QUALITY_FILE)\n",
    "df_mobility = pd.read_csv(MOBILITY_FILE)\n",
    "\n",
    "# 2. Date Conversion\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "df_aq['date'] = pd.to_datetime(df_aq['date'])\n",
    "\n",
    "# 3. Clean Mobility Data\n",
    "# The TfL file is often summarized by \"Period\". We need to approximate or filter.\n",
    "# We will create a helper column to merge on Month-Year.\n",
    "df_mobility['period_start'] = pd.to_datetime(df_mobility['Period beginning'], errors='coerce')\n",
    "# Filter for our range (roughly)\n",
    "mask = (df_mobility['period_start'] >= '2023-12-01') & (df_mobility['period_start'] <= '2024-07-01')\n",
    "df_mob_filtered = df_mobility.loc[mask].copy()\n",
    "\n",
    "# Simplify: Aggregate total journeys per period (Bus + Tube + etc)\n",
    "# Summing all transport modes (columns after specific indices usually hold the counts)\n",
    "# We will just pick 'Bus journeys (m)' and 'Underground journeys (m)' as proxies\n",
    "df_mob_filtered['total_trips'] = df_mob_filtered['Bus journeys (m)'] + df_mob_filtered['Underground journeys (m)']\n",
    "df_mob_filtered = df_mob_filtered[['period_start', 'total_trips']]\n",
    "\n",
    "# 4. Merging Strategy: Forward Fill Mobility\n",
    "# Since mobility is monthly/4-weekly, we assign the same traffic value to every day in that period.\n",
    "df_base = pd.merge(df_weather, df_aq, on='date', how='inner')\n",
    "\n",
    "# Sort to ensure proper filling\n",
    "df_base = df_base.sort_values('date')\n",
    "df_mob_filtered = df_mob_filtered.sort_values('period_start')\n",
    "\n",
    "# Merge using 'merge_asof' to find the closest previous mobility period\n",
    "df_final = pd.merge_asof(\n",
    "    df_base, \n",
    "    df_mob_filtered, \n",
    "    left_on='date', \n",
    "    right_on='period_start', \n",
    "    direction='backward'\n",
    ")\n",
    "\n",
    "# Final Cleanup\n",
    "df_final = df_final.dropna()\n",
    "df_final['day_of_week'] = df_final['date'].dt.day_name()\n",
    "\n",
    "print(f\"Final Data Shape: {df_final.shape}\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb87a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "numeric_df = df_final.select_dtypes(include=[np.number])\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix: Weather, PM2.5, and Mobility\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Time Series\n",
    "fig, ax1 = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('PM2.5 Level', color=color)\n",
    "ax1.plot(df_final['date'], df_final['pm25'], color=color, label='PM2.5')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Temperature (Â°C)', color=color)\n",
    "ax2.plot(df_final['date'], df_final['temp'], color=color, linestyle='--', label='Temp')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title(\"PM2.5 Levels vs Temperature in London (2024)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00dda10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Predict PM2.5 using Weather and Mobility data\n",
    "features = ['temp', 'humidity', 'wind_speed', 'total_trips']\n",
    "target = 'pm25'\n",
    "\n",
    "# Prepare Data\n",
    "X = df_final[features]\n",
    "y = df_final[target]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Models\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Models Trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def print_metrics(model_name, y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"[{model_name}] RMSE: {rmse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"--- Model Evaluation ---\")\n",
    "print_metrics(\"Linear Regression\", y_test, y_pred_lr)\n",
    "print_metrics(\"Random Forest    \", y_test, y_pred_rf)\n",
    "\n",
    "# Feature Importance (Random Forest)\n",
    "plt.figure(figsize=(8, 4))\n",
    "importances = pd.Series(rf_model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "importances.plot(kind='bar')\n",
    "plt.title(\"Feature Importance for Predicting PM2.5\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
